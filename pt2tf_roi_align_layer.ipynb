{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing Pytorch Torchvision MultiScale ROI_Align layer intrinsics with\n",
    "## native pytorch and tensorflow versions using pickled IO data\n",
    "\n",
    "## https://github.com/tensorflow/models/blob/master/research/...\n",
    "## .../object_detection/utils/spatial_transform_ops.py\n",
    "\n",
    "## Tensorflow Model Garden Research Utils\n",
    "# def multilevel_roi_align(features, boxes, box_levels, output_size,\n",
    "#                         num_samples_per_cell_y=1, num_samples_per_cell_x=1,\n",
    "#                         align_corners=False, extrapolation_value=0.0,\n",
    "#                         scope=None):\n",
    "\n",
    "## PyTorch TorchVision intrinsics code\n",
    "## https://github.com/pytorch/vision/blob/main/torchvision/ops/roi_align.py\n",
    "## -> https://github.com/pytorch/vision/blob/main/torchvision/csrc/ops/roi_align.cpp\n",
    "## -> https://github.com/pytorch/vision/blob/main/torchvision/csrc/ops/cpu/roi_align_kernel.cpp\n",
    "## -> https://github.com/pytorch/vision/blob/main/torchvision/csrc/ops/cpu/roi_align_common.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77da3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## Turn off warnings about missing NVinfer libs in TF\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "## try to stay off the GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "## features data from collections class, but this import isn't needed.\n",
    "# from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pickle data captured pre-post torchvision layer on live wheathead model inference.\n",
    "with open('frcnn_msroia_inputs_orig.pickle', 'rb') as handle:\n",
    "    features, boxes, image_shapes, scales, map_levels, output_size, sampling_ratio, canonical_scale, canonical_level = pickle.load(handle)\n",
    "\n",
    "with open('frcnn_msroia_outputs_orig.pickle', 'rb') as handle:\n",
    "    x_filtered, re_scales, re_map_levels, roi_out = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c092fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"features:\", type(features), len(features), features.keys(), type(features['0']), features['0'].shape)\n",
    "print(\"boxes:\", type(boxes), len(boxes), type(boxes[0]), len(boxes[0]),\n",
    "    type(boxes[0][0]), boxes[0][0].shape, boxes[0][0][0].dtype)\n",
    "print(\"image_shapes:\", type(image_shapes), len(image_shapes), type(image_shapes[0]), len(image_shapes[0]))\n",
    "print(\"scales:\", type(scales), len(scales), type(scales[0]), scales[0])\n",
    "print(\"map_levels:\", type(map_levels))\n",
    "print(\"output_size:\", type(output_size), len(output_size),\n",
    "    type(output_size[0]), type(output_size[1]),\n",
    "    output_size[0], output_size[1])\n",
    "print(\"sampling_ratio:\", type(sampling_ratio), sampling_ratio)\n",
    "print(\"canonical_scale:\", type(canonical_scale), canonical_scale)\n",
    "print(\"canonical_level:\", type(canonical_level), canonical_level)\n",
    "print(\"x_filtered:\", type(x_filtered), len(x_filtered), type(x_filtered[0]), x_filtered[0].shape)\n",
    "print(\"re_scales:\", type(re_scales), len(re_scales), type(re_scales[0]), re_scales[0])\n",
    "print(\"re_map_levels:\", type(re_map_levels))\n",
    "print(\"roi_out:\", type(roi_out), roi_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680b28f",
   "metadata": {},
   "source": [
    "# TorchVision with Intrinsics version (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bbd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This pulls up the torchvision library version!\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "roi_output = roi_pooler(features, boxes, image_shapes)\n",
    "print(\"roi_output:\", type(roi_output), roi_output.shape)\n",
    "\n",
    "## compare CPU standalone version to CUDA from embedded model layer.\n",
    "if torch.all(roi_output.eq(roi_out)):\n",
    "    print(\"Output matches exactly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73b9dc",
   "metadata": {},
   "source": [
    "# TensorFlow Model Garden MultiScale ROI Align version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defa047",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/tensorflow/models/blob/master/research/object_detection/utils/shape_utils.py\n",
    "def tf_combined_static_and_dynamic_shape(tensor):\n",
    "  static_tensor_shape = tensor.shape.as_list()\n",
    "  dynamic_tensor_shape = tf.shape(tensor)\n",
    "  combined_shape = []\n",
    "  for index, dim in enumerate(static_tensor_shape):\n",
    "    if dim is not None:\n",
    "      combined_shape.append(dim)\n",
    "    else:\n",
    "      combined_shape.append(dynamic_tensor_shape[index])\n",
    "  return combined_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa08332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_gather_valid_indices(tensor, indices, padding_value=0.0):\n",
    "  padded_tensor = tf.concat(\n",
    "      [\n",
    "          padding_value *\n",
    "          tf.ones([1, tf.shape(tensor)[-1]], dtype=tensor.dtype), tensor\n",
    "      ],\n",
    "      axis=0,\n",
    "  )\n",
    "  padded_tensor *= 1.0\n",
    "  return tf.gather(padded_tensor, indices + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_valid_indicator(feature_grid_y, feature_grid_x, true_feature_shapes):\n",
    "  height = tf.cast(true_feature_shapes[:, :, 0:1], dtype=feature_grid_y.dtype)\n",
    "  width = tf.cast(true_feature_shapes[:, :, 1:2], dtype=feature_grid_x.dtype)\n",
    "  valid_indicator = tf.logical_and(\n",
    "      tf.expand_dims(\n",
    "          tf.logical_and(feature_grid_y >= 0, tf.less(feature_grid_y, height)),\n",
    "          3),\n",
    "      tf.expand_dims(\n",
    "          tf.logical_and(feature_grid_x >= 0, tf.less(feature_grid_x, width)),\n",
    "          2))\n",
    "  return tf.reshape(valid_indicator, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81111236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_ravel_indices(feature_grid_y, feature_grid_x, num_levels, height, width,\n",
    "                  box_levels):\n",
    "  num_boxes = tf.shape(feature_grid_y)[1]\n",
    "  batch_size = tf.shape(feature_grid_y)[0]\n",
    "  size_y = tf.shape(feature_grid_y)[2]\n",
    "  size_x = tf.shape(feature_grid_x)[2]\n",
    "  height_dim_offset = width\n",
    "  level_dim_offset = height * height_dim_offset\n",
    "  batch_dim_offset = num_levels * level_dim_offset\n",
    "\n",
    "  batch_dim_indices = (\n",
    "      tf.reshape(\n",
    "          tf.range(batch_size) * batch_dim_offset, [batch_size, 1, 1, 1]) *\n",
    "      tf.ones([1, num_boxes, size_y, size_x], dtype=tf.int32))\n",
    "  box_level_indices = (\n",
    "      tf.reshape(box_levels * level_dim_offset, [batch_size, num_boxes, 1, 1]) *\n",
    "      tf.ones([1, 1, size_y, size_x], dtype=tf.int32))\n",
    "  height_indices = (\n",
    "      tf.reshape(feature_grid_y * height_dim_offset,\n",
    "                 [batch_size, num_boxes, size_y, 1]) *\n",
    "      tf.ones([1, 1, 1, size_x], dtype=tf.int32))\n",
    "  width_indices = (\n",
    "      tf.reshape(feature_grid_x, [batch_size, num_boxes, 1, size_x])\n",
    "      * tf.ones([1, 1, size_y, 1], dtype=tf.int32))\n",
    "  indices = (\n",
    "      batch_dim_indices + box_level_indices + height_indices + width_indices)\n",
    "  flattened_indices = tf.reshape(indices, [-1])\n",
    "  return flattened_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26596efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_feature_grid_coordinate_vectors(box_grid_y, box_grid_x):\n",
    "  feature_grid_y0 = tf.floor(box_grid_y)\n",
    "  feature_grid_x0 = tf.floor(box_grid_x)\n",
    "  feature_grid_y1 = tf.floor(box_grid_y + 1)\n",
    "  feature_grid_x1 = tf.floor(box_grid_x + 1)\n",
    "  feature_grid_y0 = tf.cast(feature_grid_y0, dtype=tf.int32)\n",
    "  feature_grid_y1 = tf.cast(feature_grid_y1, dtype=tf.int32)\n",
    "  feature_grid_x0 = tf.cast(feature_grid_x0, dtype=tf.int32)\n",
    "  feature_grid_x1 = tf.cast(feature_grid_x1, dtype=tf.int32)\n",
    "  return (feature_grid_y0, feature_grid_x0, feature_grid_y1, feature_grid_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_coordinate_vector_1d(start, end, size, align_endpoints):\n",
    "  start = tf.expand_dims(start, -1)\n",
    "  end = tf.expand_dims(end, -1)\n",
    "  length = end - start\n",
    "  if align_endpoints:\n",
    "    relative_grid_spacing = tf.linspace(0.0, 1.0, size)\n",
    "    offset = 0 if size > 1 else length / 2\n",
    "  else:\n",
    "    relative_grid_spacing = tf.linspace(0.0, 1.0, size + 1)[:-1]\n",
    "    offset = length / (2 * size)\n",
    "  relative_grid_spacing = tf.reshape(relative_grid_spacing, [1, 1, size])\n",
    "  relative_grid_spacing = tf.cast(relative_grid_spacing, dtype=start.dtype)\n",
    "  absolute_grid = start + offset + relative_grid_spacing * length\n",
    "  return absolute_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a601a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_box_grid_coordinate_vectors(boxes, size_y, size_x, align_corners=False):\n",
    "  ymin, xmin, ymax, xmax = tf.unstack(boxes, axis=-1)\n",
    "  box_grid_y = tf_coordinate_vector_1d(ymin, ymax, size_y, align_corners)\n",
    "  box_grid_x = tf_coordinate_vector_1d(xmin, xmax, size_x, align_corners)\n",
    "  return box_grid_y, box_grid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_pad_to_max_size(features):\n",
    "  if len(features) == 1:\n",
    "    return tf.expand_dims(features[0],\n",
    "                          1), tf.expand_dims(tf.shape(features[0])[1:3], 0)\n",
    "\n",
    "  if all([feature.shape.is_fully_defined() for feature in features]):\n",
    "    heights = [feature.shape[1] for feature in features]\n",
    "    widths = [feature.shape[2] for feature in features]\n",
    "    max_height = max(heights)\n",
    "    max_width = max(widths)\n",
    "  else:\n",
    "    heights = [tf.shape(feature)[1] for feature in features]\n",
    "    widths = [tf.shape(feature)[2] for feature in features]\n",
    "    max_height = tf.reduce_max(heights)\n",
    "    max_width = tf.reduce_max(widths)\n",
    "  features_all = [\n",
    "      tf.image.pad_to_bounding_box(feature, 0, 0, max_height,\n",
    "                                   max_width) for feature in features\n",
    "  ]\n",
    "  features_all = tf.stack(features_all, axis=1)\n",
    "  true_feature_shapes = tf.stack([tf.shape(feature)[1:3]\n",
    "                                  for feature in features])\n",
    "  return features_all, true_feature_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f0f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's build the TF version\n",
    "## https://github.com/tensorflow/models/blob/master/research/object_detection/...\n",
    "## .../utils/spatial_transform_ops.py\n",
    "\n",
    "def tf_multilevel_roi_align(features, boxes, box_levels, output_size,\n",
    "                         num_samples_per_cell_y=1, num_samples_per_cell_x=1,\n",
    "                         align_corners=False, extrapolation_value=0.0,\n",
    "                         scope=None):\n",
    "    with tf.name_scope(scope if scope is not None else 'MultiLevelRoIAlign'):\n",
    "        features, true_feature_shapes = tf_pad_to_max_size(features)\n",
    "        batch_size = tf_combined_static_and_dynamic_shape(features)[0]\n",
    "        num_levels = features.get_shape().as_list()[1]\n",
    "        max_feature_height = tf.shape(features)[2]\n",
    "        max_feature_width = tf.shape(features)[3]\n",
    "        num_filters = features.get_shape().as_list()[4]\n",
    "        num_boxes = tf.shape(boxes)[1]\n",
    "        \n",
    "        print(\"num_levels:\", num_levels)\n",
    "        print(\"num_filters:\", num_filters)\n",
    "        print(\"num_boxes:\", num_boxes)\n",
    "        ## note: num_boxes is tensor, .numpy() converts, but no behavior change\n",
    "\n",
    "        # Convert boxes to absolute co-ordinates.\n",
    "        true_feature_shapes = tf.cast(true_feature_shapes, dtype=boxes.dtype)\n",
    "        true_feature_shapes = tf.gather(true_feature_shapes, box_levels)\n",
    "        boxes *= tf.concat([true_feature_shapes - 1] * 2, axis=-1)\n",
    "\n",
    "        size_y = output_size[0] * num_samples_per_cell_y\n",
    "        size_x = output_size[1] * num_samples_per_cell_x\n",
    "        print(\"size_x, size_y:\", size_x, size_y)\n",
    "        box_grid_y, box_grid_x = tf_box_grid_coordinate_vectors(\n",
    "            boxes, size_y=size_y, size_x=size_x, align_corners=align_corners)\n",
    "        \n",
    "        # print(\"box grids:\", box_grid_y, box_grid_x) ## looks ok\n",
    "        \n",
    "        (feature_grid_y0, feature_grid_x0, feature_grid_y1,\n",
    "         feature_grid_x1) = tf_feature_grid_coordinate_vectors(box_grid_y, box_grid_x)\n",
    "        \n",
    "        feature_grid_y = tf.reshape(\n",
    "            tf.stack([feature_grid_y0, feature_grid_y1], axis=3),\n",
    "            [batch_size, num_boxes, -1])\n",
    "        feature_grid_x = tf.reshape(\n",
    "            tf.stack([feature_grid_x0, feature_grid_x1], axis=3),\n",
    "            [batch_size, num_boxes, -1])\n",
    "        \n",
    "        feature_coordinates = tf_ravel_indices(feature_grid_y, feature_grid_x,\n",
    "                                            num_levels, max_feature_height,\n",
    "                                            max_feature_width, box_levels)\n",
    "        \n",
    "        #print(\"feature_coordinates:\", feature_coordinates)\n",
    "        ## Note: 90356 int32.\n",
    "        valid_indices = tf_valid_indicator(feature_grid_y, feature_grid_x,\n",
    "                                         true_feature_shapes)\n",
    "        feature_coordinates = tf.where(valid_indices, feature_coordinates,\n",
    "                                       -1 * tf.ones_like(feature_coordinates))\n",
    "        #print(\"feature_coordinates:\", feature_coordinates)\n",
    "        \n",
    "        flattened_features = tf.reshape(features, [-1, num_filters])\n",
    "        #print(\"flattened_features:\", flattened_features)\n",
    "        ## FF is sparse array, mostly valid values, shape=(1024, 512), dtype=float32\n",
    "        \n",
    "        flattened_feature_values = tf_gather_valid_indices(flattened_features,\n",
    "                                                         feature_coordinates,\n",
    "                                                         extrapolation_value)\n",
    "        #print(\"flattened_feature_values:\", flattened_feature_values)\n",
    "        \n",
    "        \n",
    "        features_per_box = tf.reshape(\n",
    "            flattened_feature_values,\n",
    "            [batch_size, num_boxes, size_y * 2, size_x * 2, num_filters])\n",
    "\n",
    "        # Cast tensors into dtype of features.\n",
    "        box_grid_y = tf.cast(box_grid_y, dtype=features_per_box.dtype)\n",
    "        box_grid_x = tf.cast(box_grid_x, dtype=features_per_box.dtype)\n",
    "        feature_grid_y0 = tf.cast(feature_grid_y0, dtype=features_per_box.dtype)\n",
    "        feature_grid_x0 = tf.cast(feature_grid_x0, dtype=features_per_box.dtype)\n",
    "\n",
    "        ly = box_grid_y - feature_grid_y0\n",
    "        lx = box_grid_x - feature_grid_x0\n",
    "        hy = 1.0 - ly\n",
    "        hx = 1.0 - lx\n",
    "\n",
    "        kernel_y = tf.reshape(\n",
    "            tf.stack([hy, ly], axis=3), [batch_size, num_boxes, size_y * 2, 1])\n",
    "\n",
    "        kernel_x = tf.reshape(\n",
    "            tf.stack([hx, lx], axis=3), [batch_size, num_boxes, 1, size_x * 2])\n",
    "\n",
    "        # Multiplier 4 is to make tf.nn.avg_pool behave like sum_pool.\n",
    "        interpolation_kernel = kernel_y * kernel_x * 4\n",
    "\n",
    "        # Interpolate the gathered features with computed interpolation kernels.\n",
    "        features_per_box *= tf.expand_dims(interpolation_kernel, axis=4),\n",
    "        features_per_box = tf.reshape(\n",
    "            features_per_box,\n",
    "            [batch_size * num_boxes, size_y * 2, size_x * 2, num_filters])\n",
    "\n",
    "        # This combines the two pooling operations - sum_pool to perform bilinear\n",
    "        # interpolation and avg_pool to pool the values in each bin.\n",
    "        features_per_box = tf.nn.avg_pool(\n",
    "            features_per_box,\n",
    "            [1, num_samples_per_cell_y * 2, num_samples_per_cell_x * 2, 1],\n",
    "            [1, num_samples_per_cell_y * 2, num_samples_per_cell_x * 2, 1], 'VALID')\n",
    "        features_per_box = tf.reshape(\n",
    "            features_per_box,\n",
    "            [batch_size, num_boxes, output_size[0], output_size[1], num_filters])\n",
    "\n",
    "        return features_per_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BoxSize = 1024.0\n",
    "\n",
    "np_xt = features['0'].numpy()\n",
    "tf_x = [tf.transpose(tf.convert_to_tensor(np_xt), perm=[0, 2, 3, 1])]\n",
    "print(\"tf_x[0]:\", type(tf_x[0]), tf_x[0].shape)\n",
    "\n",
    "## TF ROI-Align want's normalized.\n",
    "np_boxes = boxes[0].numpy().reshape(1, -1, 4)\n",
    "tf_boxes = tf.convert_to_tensor(np_boxes / BoxSize, dtype=tf.float32)\n",
    "print(\"tf_boxes:\", type(tf_boxes), tf_boxes.shape)\n",
    "\n",
    "tf_box_levels = tf.zeros(shape=(1, tf_boxes.shape[1]), dtype=tf.int32)\n",
    "tf_output_size = [7, 7]\n",
    "\n",
    "tf_output = tf_multilevel_roi_align(tf_x, tf_boxes, tf_box_levels, tf_output_size)\n",
    "\n",
    "print(\"tf_output:\", type(tf_output), tf_output.shape)\n",
    "tptf_output = tf.transpose(tf.squeeze(tf_output, axis=0), perm=[0, 3, 1, 2]).numpy()\n",
    "## transposed tensorflow numpy\n",
    "\n",
    "## pytorch torchvision output\n",
    "print(\"roi_output:\", type(roi_output), roi_output.shape, np.sqrt(np.mean(roi_output.numpy()**2)))\n",
    "\n",
    "print(\"tptf_output:\", type(tptf_output), tptf_output.shape, np.sqrt(np.mean(tptf_output**2)))\n",
    "\n",
    "print(\"x['0']:\", type(features['0']), features['0'].shape, np.sqrt(np.mean(features['0'].numpy()**2)))\n",
    "print(\"tf_x[0]:\", type(tf_x[0]), tf_x[0].shape, np.sqrt(np.mean(tf_x[0].numpy()**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f83e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.min(np_boxes), np.max(np_boxes))\n",
    "#print(tptf_output - roi_output.numpy())\n",
    "print(np.sqrt(np.mean(roi_out.numpy()**2)), np.sqrt(np.mean(tptf_output**2)))\n",
    "A = np.count_nonzero(tptf_output - roi_out.numpy())\n",
    "B = roi_out.numel()\n",
    "print(A, \"/\", B, \"=\", str(round((100 * (B - A) / B), 3)) + \"% Matching!\")\n",
    "## Looks like TF version is very different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8537b",
   "metadata": {},
   "source": [
    "# Native Python / Torch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_initLevelMapper(\n",
    "    k_min,\n",
    "    k_max,\n",
    "    canonical_scale=224,\n",
    "    canonical_level=4,\n",
    "    eps=1e-6\n",
    "):\n",
    "    return pt_LevelMapper(k_min, k_max, canonical_scale, canonical_level, eps)\n",
    "\n",
    "\n",
    "class pt_LevelMapper:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        k_min,\n",
    "        k_max,\n",
    "        canonical_scale= 224,\n",
    "        canonical_level= 4,\n",
    "        eps= 1e-6\n",
    "    ):\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "        self.s0 = canonical_scale\n",
    "        self.lvl0 = canonical_level\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, boxlists):\n",
    "        # Compute level ids\n",
    "        s = torch.sqrt(torch.cat([box_area(boxlist) for boxlist in boxlists]))\n",
    "\n",
    "        # Eqn.(1) in FPN paper\n",
    "        target_lvls = torch.floor(self.lvl0 + torch.log2(s / self.s0) + torch.tensor(self.eps, dtype=s.dtype))\n",
    "        target_lvls = torch.clamp(target_lvls, min=self.k_min, max=self.k_max)\n",
    "        return (target_lvls.to(torch.int64) - self.k_min).to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f586664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_infer_scale(feature, original_size):\n",
    "    # assumption: the scale is of the form 2 ** (-k), with k integer\n",
    "    size = feature.shape[-2:]\n",
    "    possible_scales: List[float] = []\n",
    "    for s1, s2 in zip(size, original_size):\n",
    "        approx_scale = float(s1) / float(s2)\n",
    "        scale = 2 ** float(torch.tensor(approx_scale).log2().round())\n",
    "        possible_scales.append(scale)\n",
    "    return possible_scales[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f33a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_setup_scales(features, image_shapes, canonical_scale, canonical_level):\n",
    "    if not image_shapes:\n",
    "        raise ValueError(\"images list should not be empty\")\n",
    "    max_x = 0\n",
    "    max_y = 0\n",
    "    for shape in image_shapes:\n",
    "        max_x = max(shape[0], max_x)\n",
    "        max_y = max(shape[1], max_y)\n",
    "    original_input_shape = (max_x, max_y)\n",
    "\n",
    "    scales = [pt_infer_scale(feat, original_input_shape) for feat in features]\n",
    "    # get the levels in the feature map by leveraging the fact that the network always\n",
    "    # downsamples by a factor of 2 at each level.\n",
    "    lvl_min = -torch.log2(torch.tensor(scales[0], dtype=torch.float32)).item()\n",
    "    lvl_max = -torch.log2(torch.tensor(scales[-1], dtype=torch.float32)).item()\n",
    "\n",
    "    map_levels = pt_initLevelMapper(\n",
    "        int(lvl_min),\n",
    "        int(lvl_max),\n",
    "        canonical_scale=canonical_scale,\n",
    "        canonical_level=canonical_level,\n",
    "    )\n",
    "    return scales, map_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_filter_input(x, featmap_names):\n",
    "    x_filtered = []\n",
    "    for k, v in x.items():\n",
    "        if k in featmap_names:\n",
    "            x_filtered.append(v)\n",
    "    return x_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from torchvision/ops/_utils.py\n",
    "def pt_check_roi_boxes_shape(boxes):\n",
    "    if isinstance(boxes, (list, tuple)):\n",
    "        for _tensor in boxes:\n",
    "            torch._assert(\n",
    "                _tensor.size(1) == 4, \"The shape of the tensor in the boxes list is not correct as List[Tensor[L, 4]]\"\n",
    "            )\n",
    "    elif isinstance(boxes, torch.Tensor):\n",
    "        torch._assert(boxes.size(1) == 5, \"The boxes tensor shape is not correct as Tensor[K, 5]\")\n",
    "    else:\n",
    "        torch._assert(False, \"boxes is expected to be a Tensor[L, 5] or a List[Tensor[K, 4]]\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ad2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_convert_to_roi_format(boxes):\n",
    "    concat_boxes = torch.cat(boxes, dim=0)\n",
    "    device, dtype = concat_boxes.device, concat_boxes.dtype\n",
    "    ids = torch.cat(\n",
    "        [torch.full_like(b[:, :1],\n",
    "                         i,\n",
    "                         dtype=dtype,\n",
    "                         layout=torch.strided,\n",
    "                         device=device) for i, b in enumerate(boxes)],\n",
    "        dim=0,\n",
    "    )\n",
    "    rois = torch.cat([ids, concat_boxes], dim=1)\n",
    "    return rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f3ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppt_bilinear_interpolate_v1(\n",
    "    input,  # [N, C, H, W]\n",
    "    roi_batch_ind,  # [K]\n",
    "    y,  # [K, PH, IY]\n",
    "    x,  # [K, PW, IX]\n",
    "    ymask,  # [K, IY]\n",
    "    xmask,  # [K, IX]\n",
    "):\n",
    "    _, channels, height, width = input.size()\n",
    "\n",
    "    # deal with inverse element out of feature map boundary\n",
    "    y = y.clamp(min=0)\n",
    "    x = x.clamp(min=0)\n",
    "    y_low = y.int()\n",
    "    x_low = x.int()\n",
    "    y_high = torch.where(y_low >= height - 1, height - 1, y_low + 1)\n",
    "    y_low = torch.where(y_low >= height - 1, height - 1, y_low)\n",
    "    y = torch.where(y_low >= height - 1, y.to(input.dtype), y)\n",
    "\n",
    "    x_high = torch.where(x_low >= width - 1, width - 1, x_low + 1)\n",
    "    x_low = torch.where(x_low >= width - 1, width - 1, x_low)\n",
    "    x = torch.where(x_low >= width - 1, x.to(input.dtype), x)\n",
    "\n",
    "    ly = y - y_low\n",
    "    lx = x - x_low\n",
    "    hy = 1.0 - ly\n",
    "    hx = 1.0 - lx\n",
    "\n",
    "    # do bilinear interpolation, but respect the masking!\n",
    "    # TODO: It's possible the masking here is unnecessary if y and\n",
    "    # x were clamped appropriately; hard to tell\n",
    "    def masked_index(\n",
    "        y,  # [K, PH, IY]\n",
    "        x,  # [K, PW, IX]\n",
    "    ):\n",
    "        #print(\"NOTE: MI:\", type(x), type(y), x.dtype, y.dtype, x.shape, y.shape)\n",
    "        #print(\"roi_batch_ind:\", roi_batch_ind.dtype)\n",
    "        #print(\"channels:\", type(channels))\n",
    "        \n",
    "        if ymask is not None:\n",
    "            assert xmask is not None\n",
    "            y = torch.where(ymask[:, None, :], y, 0)\n",
    "            x = torch.where(xmask[:, None, :], x, 0)\n",
    "        return input[\n",
    "            roi_batch_ind[:, None, None, None, None, None].long(),\n",
    "            torch.arange(channels, device=input.device)[None, :, None, None, None, None].long(),\n",
    "            y[:, None, :, None, :, None].long(),  # prev [K, PH, IY]\n",
    "            x[:, None, None, :, None, :].long(),  # prev [K, PW, IX]\n",
    "        ]  # [K, C, PH, PW, IY, IX]\n",
    "\n",
    "    v1 = masked_index(y_low, x_low)\n",
    "    v2 = masked_index(y_low, x_high)\n",
    "    v3 = masked_index(y_high, x_low)\n",
    "    v4 = masked_index(y_high, x_high)\n",
    "\n",
    "    # all ws preemptively [K, C, PH, PW, IY, IX]\n",
    "    def outer_prod(y, x):\n",
    "        return y[:, None, :, None, :, None] * x[:, None, None, :, None, :]\n",
    "\n",
    "    w1 = outer_prod(hy, hx)\n",
    "    w2 = outer_prod(hy, lx)\n",
    "    w3 = outer_prod(ly, hx)\n",
    "    w4 = outer_prod(ly, lx)\n",
    "\n",
    "    val = w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9922f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppt_maybe_cast(tensor):\n",
    "    if torch.is_autocast_enabled() and tensor.is_cuda and tensor.dtype != torch.double:\n",
    "        return tensor.float()\n",
    "    else:\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e542da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pure Pytorch version 1.0\n",
    "# https://github.com/pytorch/vision/blob/main/torchvision/ops/roi_align.py\n",
    "\n",
    "def ppt_roi_align_v1(\n",
    "    input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n",
    "    orig_dtype = input.dtype\n",
    "\n",
    "    input = ppt_maybe_cast(input)\n",
    "    rois = ppt_maybe_cast(rois)\n",
    "\n",
    "    _, _, height, width = input.size()\n",
    "\n",
    "    ph = torch.arange(pooled_height, device=input.device)  # [PH]\n",
    "    pw = torch.arange(pooled_width, device=input.device)  # [PW]\n",
    "\n",
    "    # input: [N, C, H, W]\n",
    "    # rois: [K, 5]\n",
    "\n",
    "    roi_batch_ind = rois[:, 0].int()  # [K]\n",
    "    offset = 0.5 if aligned else 0.0\n",
    "    roi_start_w = rois[:, 1] * spatial_scale - offset  # [K]\n",
    "    roi_start_h = rois[:, 2] * spatial_scale - offset  # [K]\n",
    "    roi_end_w = rois[:, 3] * spatial_scale - offset  # [K]\n",
    "    roi_end_h = rois[:, 4] * spatial_scale - offset  # [K]\n",
    "\n",
    "    roi_width = roi_end_w - roi_start_w  # [K]\n",
    "    roi_height = roi_end_h - roi_start_h  # [K]\n",
    "    if not aligned:\n",
    "        roi_width = torch.clamp(roi_width, min=1.0)  # [K]\n",
    "        roi_height = torch.clamp(roi_height, min=1.0)  # [K]\n",
    "\n",
    "    bin_size_h = roi_height / pooled_height  # [K]\n",
    "    bin_size_w = roi_width / pooled_width  # [K]\n",
    "\n",
    "    exact_sampling = sampling_ratio > 0\n",
    "\n",
    "    roi_bin_grid_h = sampling_ratio if exact_sampling else torch.ceil(roi_height / pooled_height)  # scalar or [K]\n",
    "    roi_bin_grid_w = sampling_ratio if exact_sampling else torch.ceil(roi_width / pooled_width)  # scalar or [K]\n",
    "\n",
    "    \"\"\"\n",
    "    iy, ix = dims(2)\n",
    "    \"\"\"\n",
    "\n",
    "    if exact_sampling:\n",
    "        count = max(roi_bin_grid_h * roi_bin_grid_w, 1)  # scalar\n",
    "        iy = torch.arange(roi_bin_grid_h, device=input.device)  # [IY]\n",
    "        ix = torch.arange(roi_bin_grid_w, device=input.device)  # [IX]\n",
    "        ymask = None\n",
    "        xmask = None\n",
    "    else:\n",
    "        count = torch.clamp(roi_bin_grid_h * roi_bin_grid_w, min=1)  # [K]\n",
    "        # When doing adaptive sampling, the number of samples we need to do\n",
    "        # is data-dependent based on how big the ROIs are.  This is a bit\n",
    "        # awkward because first-class dims can't actually handle this.\n",
    "        # So instead, we inefficiently suppose that we needed to sample ALL\n",
    "        # the points and mask out things that turned out to be unnecessary\n",
    "        iy = torch.arange(height, device=input.device)  # [IY]\n",
    "        ix = torch.arange(width, device=input.device)  # [IX]\n",
    "        ymask = iy[None, :] < roi_bin_grid_h[:, None]  # [K, IY]\n",
    "        xmask = ix[None, :] < roi_bin_grid_w[:, None]  # [K, IX]\n",
    "\n",
    "    def from_K(t):\n",
    "        return t[:, None, None]\n",
    "\n",
    "    y = (\n",
    "        from_K(roi_start_h)\n",
    "        + ph[None, :, None] * from_K(bin_size_h)\n",
    "        + (iy[None, None, :] + 0.5) * from_K(bin_size_h / roi_bin_grid_h)\n",
    "    )  # [K, PH, IY]\n",
    "    x = (\n",
    "        from_K(roi_start_w)\n",
    "        + pw[None, :, None] * from_K(bin_size_w)\n",
    "        + (ix[None, None, :] + 0.5) * from_K(bin_size_w / roi_bin_grid_w)\n",
    "    )  # [K, PW, IX]\n",
    "    ## V1\n",
    "    val = ppt_bilinear_interpolate_v1(input, roi_batch_ind, y, x, ymask, xmask)  # [K, C, PH, PW, IY, IX]\n",
    "    ## V2\n",
    "    #n, c, ph, pw = dims(4)\n",
    "    #offset_rois = rois[n]\n",
    "    #roi_batch_ind = offset_rois[0].int()\n",
    "    #offset_input = input[roi_batch_ind.long()][c]\n",
    "    #val = ppt_bilinear_interpolate(offset_input, height, width, y, x, ymask, xmask)\n",
    "    \n",
    "    \n",
    "        # Mask out samples that weren't actually adaptively needed\n",
    "    if not exact_sampling:\n",
    "        val = torch.where(ymask[:, None, None, None, :, None], val, 0)\n",
    "        val = torch.where(xmask[:, None, None, None, None, :], val, 0)\n",
    "\n",
    "    output = val.sum((-1, -2))  # remove IY, IX ~> [K, C, PH, PW]\n",
    "    if isinstance(count, torch.Tensor):\n",
    "        output /= count[:, None, None, None]\n",
    "    else:\n",
    "        output /= count\n",
    "\n",
    "    output = output.to(orig_dtype)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.nn.modules.utils import _pair\n",
    "UseOpsVer = False\n",
    "\n",
    "def pt_roi_align(\n",
    "    input,\n",
    "    boxes,\n",
    "    output_size,\n",
    "    spatial_scale= 1.0,\n",
    "    sampling_ratio= -1,\n",
    "    aligned=False,\n",
    "):\n",
    "    \n",
    "    pt_check_roi_boxes_shape(boxes)\n",
    "    rois = boxes\n",
    "    output_size = torch.nn.modules.utils._pair(output_size)\n",
    "    if not isinstance(rois, torch.Tensor):\n",
    "        rois = pt_convert_boxes_to_roi_format(rois)\n",
    "    \n",
    "    ## This gets exact matching results.\n",
    "    if UseOpsVer:\n",
    "        return torch.ops.torchvision.roi_align(\n",
    "            input, rois, spatial_scale, output_size[0], output_size[1], sampling_ratio, aligned\n",
    "        )\n",
    "    else:\n",
    "    ## New python / torch only version, 98% matching\n",
    "    # https://github.com/pytorch/vision/blob/main/torchvision/ops/roi_align.py\n",
    "        return ppt_roi_align_v1(\n",
    "            input, rois, spatial_scale, output_size[0], output_size[1], sampling_ratio, aligned\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_multiscale_roi_align(\n",
    "    x_filtered,\n",
    "    boxes,\n",
    "    output_size,\n",
    "    sampling_ratio,\n",
    "    scales,\n",
    "    mapper):\n",
    "    \n",
    "    if scales is None or mapper is None:\n",
    "        raise ValueError(\"scales and mapper should not be None\")\n",
    "\n",
    "    num_levels = len(x_filtered)\n",
    "    rois = pt_convert_to_roi_format(boxes)\n",
    "\n",
    "    if num_levels == 1:\n",
    "        return pt_roi_align(\n",
    "            x_filtered[0],\n",
    "            rois,\n",
    "            output_size=output_size,\n",
    "            spatial_scale=scales[0],\n",
    "            sampling_ratio=sampling_ratio,\n",
    "        )\n",
    "\n",
    "    levels = mapper(boxes)\n",
    "\n",
    "    num_rois = len(rois)\n",
    "    num_channels = x_filtered[0].shape[1]\n",
    "\n",
    "    dtype, device = x_filtered[0].dtype, x_filtered[0].device\n",
    "    result = torch.zeros(\n",
    "        (\n",
    "            num_rois,\n",
    "            num_channels,\n",
    "        )\n",
    "        + output_size,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    tracing_results = []\n",
    "    for level, (per_level_feature, scale) in enumerate(zip(x_filtered, scales)):\n",
    "        idx_in_level = torch.where(levels == level)[0]\n",
    "        rois_per_level = rois[idx_in_level]\n",
    "\n",
    "        result_idx_in_level = roi_align(\n",
    "            per_level_feature,\n",
    "            rois_per_level,\n",
    "            output_size=output_size,\n",
    "            spatial_scale=scale,\n",
    "            sampling_ratio=sampling_ratio,\n",
    "        )\n",
    "    \n",
    "        result[idx_in_level] = result_idx_in_level.to(result.dtype)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From https://pytorch.org/vision/0.12/_modules/torchvision/ops/poolers.html\n",
    "## in_tools/models/vision/torchvision/ops/poolers.py \n",
    "\n",
    "class pt_MultiScaleRoIAlign(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        featmap_names,\n",
    "        output_size,\n",
    "        sampling_ratio,\n",
    "        canonical_scale=224,\n",
    "        canonical_level=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(output_size, int):\n",
    "            output_size = (output_size, output_size)\n",
    "        self.featmap_names = featmap_names\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "        self.output_size = tuple(output_size)\n",
    "        self.scales = None\n",
    "        self.map_levels = None\n",
    "        self.canonical_scale = canonical_scale\n",
    "        self.canonical_level = canonical_level\n",
    " \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        boxes,\n",
    "        image_shapes,\n",
    "    ):\n",
    "        \n",
    "        x_filtered = pt_filter_input(x, self.featmap_names)\n",
    "        if self.scales is None or self.map_levels is None:\n",
    "            self.scales, self.map_levels = pt_setup_scales(\n",
    "                x_filtered, image_shapes, self.canonical_scale, self.canonical_level\n",
    "            )\n",
    "\n",
    "        return pt_multiscale_roi_align(\n",
    "            x_filtered,\n",
    "            boxes,\n",
    "            self.output_size,\n",
    "            self.sampling_ratio,\n",
    "            self.scales,\n",
    "            self.map_levels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2246746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This brings up local pytorch version!\n",
    "pt_roi_pooler = pt_MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "pt_roi_output = pt_roi_pooler(features, boxes, image_shapes)\n",
    "print(\"pt_roi_output:\", type(pt_roi_output), pt_roi_output.shape)\n",
    "\n",
    "if torch.all(pt_roi_output.eq(roi_out)):\n",
    "    print(\"Output matches exactly!\")\n",
    "else:\n",
    "    print(\"Output is NOT the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pt_roi_output - roi_out)\n",
    "print(np.sqrt(np.mean(roi_output.numpy()**2)), np.sqrt(np.mean(pt_roi_output.numpy()**2)))\n",
    "A = torch.count_nonzero(pt_roi_output - roi_out).numpy()\n",
    "B = roi_out.numel()\n",
    "print(A, \"/\", B, \"=\", str(round((100 * (B - A) / B), 3)) + \"% Matching!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae7cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81570de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
